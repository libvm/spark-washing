{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first function you have to implement. You are passed a dataframe object. We've also registered the dataframe in the ApacheSparkSQL catalog - so you can also issue queries against the \"washing\" table using \"spark.sql()\". Hint: To get an idea about the contents of the catalog you can use: spark.catalog.listTables().\n",
    "So now it's time to implement your first function. You are free to use the dataframe API, SQL or RDD API. In case you want to use the RDD API just obtain the encapsulated RDD using \"df.rdd\". You can test the function by running one of the three last cells of this notebook, but please make sure you run the cells from top to down since some are dependant of each other..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count():\n",
    "    return spark.sql('SELECT COUNT(*) AS cnt FROM washing').first().cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to implement the second function. Please return an integer containing the number of fields (columns). The most easy way to get this is using the dataframe API. Hint: You might find the dataframe API documentation useful: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumberOfFields():\n",
    "    return len(spark.table(\"washing\").columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, please implement a function which returns a (python) list of string values of the field names in this data frame. Hint: Just copy&past doesn't work because the auto-grader will create a random data frame for testing, so please use the data frame API as well. Again, this is the link to the documentation: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFieldNames():\n",
    "    return spark.table(\"washing\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to grab a PARQUET file and create a dataframe out of it. Using SparkSQL you can handle it like a database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------+----------+---------+--------+-----+-----------+-------------+-------+\n",
      "|                 _id|                _rev|count|flowrate|fluidlevel|frequency|hardness|speed|temperature|           ts|voltage|\n",
      "+--------------------+--------------------+-----+--------+----------+---------+--------+-----+-----------+-------------+-------+\n",
      "|0d86485d0f88d1f9d...|1-57940679fb8a713...|    4|      11|acceptable|     NULL|      77| NULL|        100|1547808723923|   NULL|\n",
      "|0d86485d0f88d1f9d...|1-15ff3a0b304d789...|    2|    NULL|      NULL|     NULL|    NULL| 1046|       NULL|1547808729917|   NULL|\n",
      "|0d86485d0f88d1f9d...|1-97c2742b68c7b07...|    4|    NULL|      NULL|       71|    NULL| NULL|       NULL|1547808731918|    236|\n",
      "|0d86485d0f88d1f9d...|1-eefb903dbe45746...|   19|      11|acceptable|     NULL|      75| NULL|         86|1547808738999|   NULL|\n",
      "|0d86485d0f88d1f9d...|1-5f68b4c72813c25...|    7|    NULL|      NULL|       75|    NULL| NULL|       NULL|1547808740927|    235|\n",
      "|0d86485d0f88d1f9d...|1-cd4b6c57ddbe77e...|    5|    NULL|      NULL|     NULL|    NULL| 1014|       NULL|1547808744923|   NULL|\n",
      "|0d86485d0f88d1f9d...|1-a35b25b5bf43aaf...|   32|      11|acceptable|     NULL|      73| NULL|         84|1547808752028|   NULL|\n",
      "|0d86485d0f88d1f9d...|1-b717f7289a8476d...|   48|      11|acceptable|     NULL|      79| NULL|         84|1547808768065|   NULL|\n",
      "|0d86485d0f88d1f9d...|1-c2f1f8fcf178b2f...|   18|    NULL|      NULL|       73|    NULL| NULL|       NULL|1547808773944|    228|\n",
      "|0d86485d0f88d1f9d...|1-15033dd9eebb4a8...|   59|      11|acceptable|     NULL|      72| NULL|         96|1547808779093|   NULL|\n",
      "|0d86485d0f88d1f9d...|1-753dae825f9a6c2...|   62|      11|acceptable|     NULL|      73| NULL|         88|1547808782113|   NULL|\n",
      "|0d86485d0f88d1f9d...|1-b168089f44f03f0...|   13|    NULL|      NULL|     NULL|    NULL| 1097|       NULL|1547808784940|   NULL|\n",
      "|0d86485d0f88d1f9d...|1-403b687c6be0dea...|   23|    NULL|      NULL|       80|    NULL| NULL|       NULL|1547808788955|    236|\n",
      "|0d86485d0f88d1f9d...|1-195551e0455a24b...|   72|      11|acceptable|     NULL|      77| NULL|         87|1547808792134|   NULL|\n",
      "|0d86485d0f88d1f9d...|1-060a39fc6c2ddee...|   26|    NULL|      NULL|       62|    NULL| NULL|       NULL|1547808797959|    233|\n",
      "|0d86485d0f88d1f9d...|1-2234514bffee465...|   27|    NULL|      NULL|       61|    NULL| NULL|       NULL|1547808800960|    226|\n",
      "|0d86485d0f88d1f9d...|1-4265898bb401db0...|   82|      11|acceptable|     NULL|      79| NULL|         96|1547808802154|   NULL|\n",
      "|0d86485d0f88d1f9d...|1-2fbf7ca9a0425a0...|   94|      11|acceptable|     NULL|      73| NULL|         90|1547808814186|   NULL|\n",
      "|0d86485d0f88d1f9d...|1-203c0ee6d7fbd21...|   97|      11|acceptable|     NULL|      77| NULL|         88|1547808817190|   NULL|\n",
      "|0d86485d0f88d1f9d...|1-47e1965db94fcab...|  104|      11|acceptable|     NULL|      75| NULL|         80|1547808824198|   NULL|\n",
      "+--------------------+--------------------+-----+--------+----------+---------+--------+-----+-----------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('washing.parquet')\n",
    "df.createOrReplaceTempView('washing')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell can be used to test your count function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2058\n"
     ]
    }
   ],
   "source": [
    "cnt = None\n",
    "nof = None\n",
    "fn = None\n",
    "\n",
    "cnt = count()\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell can be used to test your getNumberOfFields function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "nof = getNumberOfFields()\n",
    "print(nof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following cell can be used to test your getFieldNames function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_id', '_rev', 'count', 'flowrate', 'fluidlevel', 'frequency', 'hardness', 'speed', 'temperature', 'ts', 'voltage']\n"
     ]
    }
   ],
   "source": [
    "fn = getFieldNames()\n",
    "print(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Congratulations, you are done. So please submit your solutions now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
